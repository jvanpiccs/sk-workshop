{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "####  Lab 04b: Semantic Advanced Topics (Chat with your data Scenario using Vector Search)\n",
    "\n",
    "In this lab, we will learn how to create a virtual agent using Semantic Kernel, Azure OpenAI models and a AI Search as a Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Resources**\n",
    "\n",
    "First create an Azure OpenAI service with a gpt-4 or gpt-35-turbo deployment using Azure Portal. For a better performance use a gpt-4 model.\n",
    "\n",
    "In the same Azure OpenAI service create an text-embedding-ada-002 version 2 model deployment with text-embedding-ada-002 name.\n",
    "\n",
    "Finally create an Azure AI Search service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Configuration**\n",
    "\n",
    "Configure the environment variables that Semantic Kernel will use to connect to this service by creating an **.env** file. \n",
    "\n",
    "You can use **.env.template** as a template for your **.env** file, just rename it and replace the variables accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the Kernel**\n",
    "\n",
    "Initialize the kernel and register a persistent **Semantic Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.core_skills import ConversationSummarySkill, TextMemorySkill\n",
    "import time\n",
    "\n",
    "# initalize and immport TextMemorySkill\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "deployment, api_key, endpoint, api_version = sk.azure_openai_settings_from_dot_env(include_api_version=True)\n",
    "kernel.add_chat_service(\"chat-completion\", AzureChatCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "kernel.add_text_embedding_generation_service(\"ada\",AzureTextEmbedding(\"text-embedding-ada-002\", endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "\n",
    "# register AI Search as a memory store\n",
    "\n",
    "azure_ai_search_api_key, azure_ai_search_url = sk.azure_aisearch_settings_from_dot_env()\n",
    "kernel.register_memory_store(\n",
    "    memory_store=AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=1536,\n",
    "        search_endpoint=azure_ai_search_url,\n",
    "        admin_key=azure_ai_search_api_key\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RAGChat plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p plugins/RAGChat/Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plugins/RAGChat/Chat/config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile plugins/RAGChat/Chat/config.json\n",
    "{\n",
    "     \"schema\": 1,\n",
    "     \"type\": \"completion\",\n",
    "     \"description\": \"Based on the user ask, conversation history, search the memory for sources and answer the user.\",\n",
    "     \"completion\": {\n",
    "          \"max_tokens\": 200,\n",
    "          \"temperature\": 0.8,\n",
    "          \"top_p\": 0.0,\n",
    "          \"presence_penalty\": 0.0,\n",
    "          \"frequency_penalty\": 0.0\n",
    "     },\n",
    "     \"input\": {\n",
    "          \"parameters\": [\n",
    "               {\n",
    "                    \"name\": \"ask\",\n",
    "                    \"description\": \"The user's ask.\",\n",
    "                    \"defaultValue\": \"\",\n",
    "                    \"required\": true\n",
    "               },\n",
    "               {\n",
    "                    \"name\": \"chat_history\",\n",
    "                    \"description\": \"The conversation history.\",\n",
    "                    \"defaultValue\": \"\",\n",
    "                    \"required\": true\n",
    "               }                \n",
    "          ]\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plugins/RAGChat/Chat/skprompt.txt\n"
     ]
    }
   ],
   "source": [
    "## Task Goal\n",
    "The task goal is to generate an ANSWER based on the user QUESTION and the provided SOURCES.\n",
    " \n",
    "## Task instructions\n",
    " \n",
    "You will be given a list of SOURCES that you can use to ANSWER the QUESTION. \n",
    "You will be given a conversation HISTORY to give you more context. \n",
    "You must use the SOURCES to ANSWER the QUESTION. \n",
    "You must not use any other SOURCES.\n",
    "You must not use your own knowledge to ANSWER the QUESTION.\n",
    "Do not include the word \"ANSWER\" in your response.\n",
    "Always include the SOURCE name for each fact in the response, referencing it with square brackets, e.g., [info1.txt]. \n",
    "Do not combine SOURCES; list each source separately, e.g., [info1.txt][info2.pdf].\n",
    "\n",
    "## Task Input:\n",
    "\"QUESTION\": \"{{$ask}}\"\n",
    "\"HISTORY\": \"{{ConversationSummaryPlugin.SummarizeConversation $chat_history}}\"\n",
    "\"SOURCES\": \"{{TextMemoryPlugin.recall $ask}}\"\n",
    " \n",
    "## Task Output:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents into memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading documents into memory\")\n",
    "\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/overview/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/] Effective prompt design is essential to achieving desired outcomes with LLM AI models. Prompt engineering, also known as prompt design, is an emerging field that requires creativity and attention to detail. It involves selecting the right words, phrases, symbols, and formats that guide the model in generating high-quality and relevant texts.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/] A GPT model is a type of neural network that uses the transformer architecture to learn from large amounts of text data. The model has two main components: an encoder and a decoder. The encoder processes the input text and converts it into a sequence of vectors, called embeddings, that represent the meaning and context of each word. The decoder generates the output text by predicting the next word in the sequence, based on the embeddings and the previous words. The model uses a technique called attention to focus on the most relevant parts of the input and output texts, and to capture long-range dependencies and relationships between words. The model is trained by using a large corpus of texts as both the input and the output, and by minimizing the difference between the predicted and the actual words. The model can then be fine-tuned or adapted to specific tasks or domains, by using smaller and more specialized datasets.\"\n",
    ")\n",
    "await kernel.memory.save_information_async(\n",
    "    \"kb\", id=\"https://learn.microsoft.com/en-us/semantic-kernel/memories/\", text=\"[https://learn.microsoft.com/en-us/semantic-kernel/memories/] Embeddings are a way of representing words or other data as vectors in a high-dimensional space. Vectors are like arrows that have a direction and a length. High-dimensional means that the space has many dimensions, more than we can see or imagine. The idea is that similar words or data will have similar vectors, and different words or data will have different vectors. This helps us measure how related or unrelated they are, and also perform operations on them, such as adding, subtracting, multiplying, etc. Embeddings are useful for AI models because they can capture the meaning and context of words or data in a way that computers can understand and process.\"\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plugins and initialize context\n",
    "\n",
    "text_memory_skill = kernel.import_skill(TextMemorySkill(), skill_name=\"TextMemoryPlugin\")\n",
    "conversation_summary_plugin = kernel.import_skill(ConversationSummarySkill(kernel=kernel), skill_name=\"ConversationSummaryPlugin\")\n",
    "ragchat_plugin = kernel.import_semantic_skill_from_directory(\"./plugins\", \"RAGChat\")\n",
    "\n",
    "context = kernel.create_new_context()\n",
    "context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"kb\"\n",
    "context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "context[sk.core_skills.TextMemorySkill.LIMIT_PARAM] = 3\n",
    "context[\"chat_history\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test memory search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: [\"[https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\", \"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/] Effective prompt design is essential to achieving desired outcomes with LLM AI models. Prompt engineering, also known as prompt design, is an emerging field that requires creativity and attention to detail. It involves selecting the right words, phrases, symbols, and formats that guide the model in generating high-quality and relevant texts.\", \"[https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models/] A GPT model is a type of neural network that uses the transformer architecture to learn from large amounts of text data. The model has two main components: an encoder and a decoder. The encoder processes the input text and converts it into a sequence of vectors, called embeddings, that represent the meaning and context of each word. The decoder generates the output text by predicting the next word in the sequence, based on the embeddings and the previous words. The model uses a technique called attention to focus on the most relevant parts of the input and output texts, and to capture long-range dependencies and relationships between words. The model is trained by using a large corpus of texts as both the input and the output, and by minimizing the difference between the predicted and the actual words. The model can then be fine-tuned or adapted to specific tasks or domains, by using smaller and more specialized datasets.\"]\n",
      "Memory search: [https://learn.microsoft.com/en-us/semantic-kernel/overview/] Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python to build LLM AI models. By doing so, you can create AI apps that combine the best of both worlds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask = \"What are LLM AI Apps?\"\n",
    "\n",
    "# first test the recall function.\n",
    "variables = sk.ContextVariables()\n",
    "variables[\"collection\"] = \"kb\"\n",
    "variables[\"relevance\"] = 0.8\n",
    "variables[\"limit\"] = 3\n",
    "variables[\"input\"] = ask\n",
    "output_context = await kernel.run_async(\n",
    "    text_memory_skill[\"recall\"],\n",
    "    input_vars = variables\n",
    ")\n",
    "if output_context.error_occurred:\n",
    "    print(output_context.last_error_description)\n",
    "else:\n",
    "    print(\"Recall:\", output_context.result)\n",
    "\n",
    "# memory search\n",
    "result = await kernel.memory.search_async(\"kb\", ask)\n",
    "print(f\"Memory search: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create chat conversation loop    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.SKContext\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"ask\"] = user_input\n",
    "        print(f\"User:> {user_input}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "    if answer.error_occurred:\n",
    "        answer = answer.last_error_description\n",
    "    \n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:> hello\n",
      "ChatBot:> Since there are no SOURCES provided and the QUESTION \"hello\" does not request specific information, a factual answer cannot be generated. If you have a specific question or need information, please provide more context or sources for reference.\n",
      "User:> what is semanti kernel?\n",
      "ChatBot:> Semantic Kernel is an open-source Software Development Kit (SDK) that enables the integration of AI services such as OpenAI, Azure OpenAI, and Hugging Face with traditional programming languages, including C# and Python. This integration facilitates the development of Large Language Model (LLM) AI applications that leverage the strengths of both AI services and conventional programming approaches [https://learn.microsoft.com/en-us/semantic-kernel/overview/].\n",
      "User:> and who is pele?\n",
      "ChatBot:> Since there are no SOURCES provided, I cannot give you a sourced answer about who Pelé is. If you can provide a source, I'd be happy to help with information from it.\n",
      "User:> ok\n",
      "ChatBot:> Since there are no SOURCES provided, I am unable to offer any factual information or answer your question. Please provide a source or more context so I can assist you effectively.\n",
      "User:> exit\n",
      "\n",
      "\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await chat(kernel, ragchat_plugin[\"Chat\"], context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ChatBot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ChatBot.py\n",
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore\n",
    "from semantic_kernel.core_skills import ConversationSummarySkill, TextMemorySkill\n",
    "import asyncio\n",
    "\n",
    "# initalize and immport TextMemorySkill\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "\n",
    "deployment, api_key, endpoint, api_version = sk.azure_openai_settings_from_dot_env(include_api_version=True)\n",
    "kernel.add_chat_service(\"chat-completion\", AzureChatCompletion(deployment_name=deployment, endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "kernel.add_text_embedding_generation_service(\"ada\",AzureTextEmbedding(\"text-embedding-ada-002\", endpoint=endpoint, api_key=api_key, api_version=api_version))\n",
    "\n",
    "# register AI Search as a memory store\n",
    "\n",
    "azure_ai_search_api_key, azure_ai_search_url = sk.azure_aisearch_settings_from_dot_env()\n",
    "kernel.register_memory_store(\n",
    "    memory_store=AzureCognitiveSearchMemoryStore(\n",
    "        vector_size=1536,\n",
    "        search_endpoint=azure_ai_search_url,\n",
    "        admin_key=azure_ai_search_api_key\n",
    "    )\n",
    ")\n",
    "\n",
    "# Import plugins and initialize context\n",
    "\n",
    "text_memory_skill = kernel.import_skill(TextMemorySkill(), skill_name=\"TextMemoryPlugin\")\n",
    "conversation_summary_plugin = kernel.import_skill(ConversationSummarySkill(kernel=kernel), skill_name=\"ConversationSummaryPlugin\")\n",
    "ragchat_plugin = kernel.import_semantic_skill_from_directory(\"./plugins\", \"RAGChat\")\n",
    "\n",
    "context = kernel.create_new_context()\n",
    "context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"kb\"\n",
    "context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "context[sk.core_skills.TextMemorySkill.LIMIT_PARAM] = 3\n",
    "context[\"chat_history\"] = \"\"\n",
    "\n",
    "# Chat flow\n",
    "\n",
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.SKContext\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"ask\"] = user_input\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    \n",
    "    except Exception:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.run_async(chat_func, input_vars=context.variables)\n",
    "    if answer.error_occurred:\n",
    "        answer = answer.last_error_description\n",
    "    \n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "async def main():\n",
    "    chatting = True\n",
    "    while chatting:\n",
    "        chatting = await chat(kernel, ragchat_plugin[\"Chat\"], context)\n",
    "\n",
    "# Create an event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "# Use the event loop to run the main function until it completes\n",
    "loop.run_until_complete(main())\n",
    "# Close the loop\n",
    "loop.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this on your terminal:\n",
    "\n",
    "```\n",
    "cd lesson_04\n",
    "python src/ChatBot.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
