{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB01B - Introduction to Semantic Kernel\n",
    "\n",
    "In this lab, we will learn how to use Semantic Kernel to interact with Large Language Models. We'll complete the following steps:\n",
    "\n",
    "1) Import general dependencies.\n",
    "2) Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI. \n",
    "3) Instantiate a Kernel to add LLM capabilities to your application.\n",
    "4) Connect the Kernel to a Completion model to run your first semantic function.\n",
    "\n",
    "**Important:** We're a using a method to automatically read sensitive parameters from a `.env` file. Therefore, please ensure that you have a `.env` file in the same directory of this notebook with the following parameters:\n",
    "\n",
    "``` python\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR MODEL DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\"\n",
    "```\n",
    "Optionally, you can update the values and run the cell below to create your `.env` file. Please note the deployment name as we'll use both Text Completion and Chat Completion in this LAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR TEXT COMPLETION DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Import Dependencies\n",
    "\n",
    "**Semantic Kernel** is a SDK currently available in C#, Java and Python. However, each language support different at the current stage. Please visit [supported languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to check which features are supported per language.\n",
    "\n",
    "In this lab, we'll use Python SDK which can be obtained in Pypi repository.\n",
    "\n",
    "**Important:** if you get an error running the code below, make sure that you've completed the setup process. Optionally, you can run `pip install semantic_kernel` in your terminal or `!python -m pip install semantic_kernel` in a Python code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "#The other dependencies will be imported directly in the step where they are used.\n",
    "import semantic_kernel as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Using Semantic Kernel SDK to run prompts on a Text Completion model from Azure OpenAI\n",
    "\n",
    "In this step, we'll connect to an **Azure OpenAI Service** and use a Text Completion model. \n",
    "The Semantic Kernel also supports **OpenAI**. Other AI Services, like **Hugging Face** will be supported shortly. You can visit this [link](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages#ai-service-endpoints) to check the most updated list.\n",
    "\n",
    "In this example, we're using **AzureTextCompletion** module to interact with Text Completion models from Azure OpenAI, such as `gpt-35-turbo-instruct` or `text-davinci-003`. This is suitable for text generation activities. We are also using a module to control the behavior of the requests, defining parameters like Temperature, max_tokens and even how many responses we want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Azure Text Completion service connector\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion\n",
    "#Importing the module that contains the settings for the Azure Text Completion service\n",
    "from semantic_kernel.connectors.ai import CompleteRequestSettings\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "# Create the Azure Text Completion service connector\n",
    "azure_text_completion = AzureTextCompletion(deployment, endpoint, api_key)\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=20,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all set, let's run a simple prompt to generate taglines using the Text Completion model. Please note that the number of results, response lenght and temperature are controlled by the **CompleteRequestSettings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write tagline for a Telecom company.\"\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "results = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "#Printing out the results\n",
    "i = 1\n",
    "for result in results:\n",
    "    print(f\"Result {i}: {result}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to use the SDK to run a prompt, let's start building out our Call Center Analytics Solution. We have already explored how to use prompts to summarize and extract relevant information from call center conversations. As our next step, let's run them in Semantic Kernel. We'll explore different methods.\n",
    "\n",
    "We'll use the conversation below as an example. If you prefer, just change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a call center conversation between a customer and an agent. You can replace by your own example\n",
    "input = \"\"\"\n",
    "Agente: Hola, bienvenido al servicio de atención al cliente de Telecom. Mi nombre es Juan, ¿en qué puedo ayudarlo?\n",
    "Cliente: Hola, Juan. Estoy llamando porque tengo problemas con mi plan de datos móviles. Es muy lento y no puedo navegar por internet ni usar mis aplicaciones.\n",
    "Agente: Lo siento mucho por las molestias, señor. ¿Me puede decir su número de teléfono y su nombre completo, por favor?\n",
    "Cliente: Sí, claro. Mi número es 011-4567-8910 y mi nombre es Martín Pérez.\n",
    "Agente: Gracias, señor Pérez. Voy a verificar su plan y su consumo de datos. Un momento, por favor.\n",
    "Cliente: Está bien, gracias.\n",
    "Agente: Señor Pérez, he revisado su plan y veo que tiene contratado el plan básico de 2 GB de datos por mes. ¿Es correcto?\n",
    "Cliente: Sí, así es.\n",
    "Agente: Bueno, le informo que ha consumido el 90% de su límite de datos y que le quedan solo 200 MB disponibles hasta el final del mes. Por eso su velocidad de navegación se ha reducido.\n",
    "Cliente: ¿Qué? ¿Cómo es posible? Yo apenas uso el internet en mi celular. Solo reviso mi correo y mis redes sociales de vez en cuando. No veo videos ni descargo archivos grandes.\n",
    "Agente: Entiendo, señor Pérez. Pero tenga en cuenta que algunas aplicaciones consumen datos en segundo plano, sin que usted se dé cuenta. Por ejemplo, las actualizaciones automáticas, las copias de seguridad, el GPS, etc.\n",
    "Cliente: Bueno, pero eso no me lo explicaron cuando contraté el plan. Me dijeron que con 2 GB tendría suficiente para todo el mes. Me siento engañado.\n",
    "Agente: Le pido disculpas, señor Pérez. No era nuestra intención engañarlo. Le ofrezco una solución: si quiere, puede cambiar su plan a uno superior, con más GB de datos y mayor velocidad. Así podrá disfrutar de una mejor experiencia de navegación.\n",
    "Cliente: ¿Y cuánto me costaría eso?\n",
    "Agente: Tenemos una oferta especial para usted. Por solo 10 pesos más al mes, puede acceder al plan premium de 5 GB de datos y velocidad 4G. ¿Le interesa?\n",
    "Cliente: Mmm, no sé. ¿No hay otra opción? ¿No pueden darme más velocidad sin cobrarme más?\n",
    "Agente: Lo siento, señor Pérez. Esa es la única opción que tenemos disponible. Si no cambia su plan, tendrá que esperar hasta el próximo mes para recuperar su velocidad normal. O puede comprar un paquete adicional de datos, pero le saldría más caro que cambiar de plan.\n",
    "Cliente: Bueno, déjeme pensarlo. ¿Puedo llamar más tarde para confirmar?\n",
    "Agente: Claro, señor Pérez. Puede llamar cuando quiera. El número es el mismo que marcó ahora. ¿Hay algo más en lo que pueda ayudarlo?\n",
    "Cliente: No, eso es todo. Gracias por su atención.\n",
    "Agente: Gracias a usted, señor Pérez. Que tenga un buen día. Adiós.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same prompt for \"Problem Summarization\". Note that we're using python strings to append the Conversation to our prompt. As you run this cell, we should see a summary similar to what we saw in Azure AI Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially, let's use a prompt similar to what we have tested in the AI Studio.\n",
    "prompt = f\"\"\"\n",
    "Summarize the problem described below. \n",
    "Be brief, yet informative.\n",
    "Try to capture in the summary:\n",
    "- The problem description;\n",
    "- The product customer is complaining (if available);\n",
    "- How Agente, handled the call;\n",
    "- The outcome of the call. \n",
    "\n",
    "[INPUT]\n",
    "{input}\n",
    "[END INPUT]\n",
    "\"\"\"\n",
    "\n",
    "# Create the settings to control the behavior to the requests of Azure Text Completion service\n",
    "request_settings = CompleteRequestSettings(\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    number_of_responses=1\n",
    ")\n",
    "\n",
    "#Calling the Azure Text Completion service using Semantic Kernel SDK and AI Service Connector\n",
    "result = await azure_text_completion.complete_async(prompt, request_settings)\n",
    "\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Instantiate a Kernel to add LLM capabilities to your application\n",
    "The core of Semantic Kernel is it's Kernel. It's responsible to transform an User Input (or ASK) into a response by orchestrating Plugins. It's also what makes Semantic Kernel shines as it can transform existing applications in AI applications powered by LLMs. \n",
    "\n",
    "In this step, we'll instantiate a simple Kernel with no additonal options. Optionally, we could define Logging, Memory and the set of skills we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "\n",
    "#Example of how to instantiate the kernel with a different log level\n",
    "#logger = sk.NullLogger()\n",
    "#kernel_with_logging = sk.Kernel(log=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Connect the Kernel to a Completion model to run your first semantic function.\n",
    "Now that we have our kernel, we can start adding capabilities to it. We already saw how to use the SDK and connectors to the AI services to run prompts. Now, let's see how can we use the Kernel to transform a prompt into a semantic function able to run Text Completion activities using a LLM.\n",
    "\n",
    "The first step is to add to our kernel the Text Completion model that we already have defined before. We can add multiple models and give then names (or nicknames) to later decide which ones we want to use and define programmatically which one is the default.\n",
    "\n",
    "You can also add Chat Completion models, like GPT-4 and GPT-3.5-Turbo, as per the commented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the text completion service to the kernel\n",
    "kernel.add_text_completion_service(\"Text Completion\", \n",
    "                                   AzureTextCompletion(deployment, endpoint, api_key))\n",
    "\n",
    "#We can add several models of the same type to the kernel\n",
    "#kernel.add_text_completion_service(\"Other Text Completion\",\n",
    "#                                   AzureTextCompletion(\"text-curie-001\", endpoint, api_key))\n",
    "\n",
    "#We can add other models types to the kernel, like ChatCompletions (GPT-35-Turbo, GPT4)\n",
    "#from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "#kernel.add_chat_completion_service(\"ChatCompletion - GPT-4\",\n",
    "#                                   AzureChatCompletionCompletion(\"text-curie-001\", endpoint, api_key))\n",
    "\n",
    "#kernel.set_default_text_completion_service(\"Text Completion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kernel connected to a large language model from Azure OpenAI Service, we can start using it to run Text Completion operations. The best way to make our prompts reusable is to create a **Semantic Function**. You can create and add one to the Kernel based on a **Prompt**. This is simple a textual description, or instruction, of what we want the function to do. Optionally, we can add parameters, like `{{$input}}`, to inject values in runtime.\n",
    "\n",
    "We'll deep dive in Semantic Functions later and now we'll demonstrate a simplified way to create and run Semantic Function **inline**. With this method we can start structuring our application and transforming Prompts into functions. Please note that as we define the function we can also control the behavior of how we expect the kernel to run it, for example, controlling the max number of tokens in the response and the temperature. This we'll ensure that the function we'll run consistenly.\n",
    "\n",
    "Therefore, in this step, we'll create an **inline semantic function** to classify a Telecom problem in different categories, like Mobile Internet, Fixed Internet, etc. This is based on the same example we run in Azure AI Studio and is one of the things we need to complete in our Call Center Analytics Solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt for categorizing based on the problem description\n",
    "prompt = \"\"\"\n",
    "Problem Description: {{$input}}\n",
    "Classify the problem description above as one of these categories: Fixed Internet, Mobile Internet, TV, Landline, Billing.\n",
    "Only write the output category with no extra text.\n",
    "Only write one category per problem description.\n",
    "\"\"\"\n",
    "\n",
    "# Create a semantic function for classifying the problem description\n",
    "classify_call_function = kernel.create_semantic_function(prompt, \n",
    "                                                         max_tokens=10,\n",
    "                                                         temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Kernel has a new function to Categorize problems, we'll test it based on a problem description. We expected as a result one of the classifications previously defined in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the classify function with a problem description\n",
    "classification = classify_call_function(input)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Result: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Running Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR CHAT COMPLETION DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run Chat Completion Operations with Semantic Kernel. Let's see a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai import CompleteRequestSettings\n",
    "\n",
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "kernel.add_chat_service(\"Chat Completion\", \n",
    "                        AzureChatCompletion(deployment, endpoint, api_key))\n",
    "\n",
    "#Define a system message to model how the model should behave\n",
    "system_message = \"\"\"\n",
    "You're an AI assistant that helps Telecom company to extract valuable information from their conversations by creating JSON files for each conversation transcription you receive. You always try to extract and format as a JSON:\n",
    "1. Customer Name [name]\n",
    "2. Customer Contact Phone [phone]\n",
    "3. Main Topic of the Conversation [topic]\n",
    "4. Customer Sentiment (Neutral, Positive, Negative)[sentiment]\n",
    "5. How the Agent Handled the Conversation [agent_behavior]\n",
    "6. What was the FINAL Outcome of the Conversation [outcome]\n",
    "7. A really brief Summary of the Conversation [summary]\n",
    "\n",
    "Only extract information that you're sure. If you're unsure, write \"Unknown/Not Found\" in the JSON file.\n",
    "\"\"\"\n",
    "\n",
    "#Prepare the prompt template to be used by the semantic function\n",
    "prompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n",
    "    max_tokens=500, temperature=0.7, top_p=0.5\n",
    ")\n",
    "prompt_template = sk.ChatPromptTemplate(\n",
    "    \"{{$input}}\", kernel.prompt_template_engine, prompt_config\n",
    ")\n",
    "\n",
    "#Ensure our prompt template have at least a System Message\n",
    "prompt_template.add_system_message(system_message)\n",
    "#Optionally, we can load user and assistant messages to use as Few Shot examples\n",
    "prompt_template.add_user_message(\"Hello, how are you?\")\n",
    "prompt_template.add_assistant_message(\"Hello! I can help you to extract valuable information from Call Center conversations.\")\n",
    "\n",
    "extract_function_config = sk.SemanticFunctionConfig(prompt_config, prompt_template)\n",
    "extract_fn = kernel.register_semantic_function(\"CallCenter\", \"ChatExtract\", extract_function_config)\n",
    "context_vars = sk.ContextVariables()\n",
    "try:\n",
    "    context_vars[\"input\"] = input(\"Enter a Conversation Transcription:> \")\n",
    "    result = await kernel.run_async(extract_fn, input_vars=context_vars)\n",
    "    # Print the result\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
