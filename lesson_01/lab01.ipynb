{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Semantic Kernel\n",
    "\n",
    "In this lab, we will learn how to use Semantic Kernel to interact with Large Language Models. We'll complete the following steps:\n",
    "\n",
    "1) Import general dependencies.\n",
    "2) Instantiate the Kernel.\n",
    "3) Connect the Kernel to a Completion model and run a simple prompt.\n",
    "4) Connect the Kernel to a Chat Completion model and run a simple prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - Import Dependecies\n",
    "\n",
    "**Semantic Kernel** is a SDK currently available in C#, Java and Python. However, each language support different at the current stage. Please visit [supported languages](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages) to check which features are supported per language.\n",
    "\n",
    "In this lab, we'll use Python SDK which can be obtained in Pypi repository.\n",
    "\n",
    "**Important:** if you get an error running the code below, make sure that you've completed the setup process. Optionally, you can run `pip install semantic_kernel` in your terminal or `!python -m pip install semantic_kernel` in a Python code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Semantic Kernel SDK that will be used in this lab. \n",
    "#The other dependencies will be imported directly in the step where they are used.\n",
    "import semantic_kernel as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - Instantiate the Kernel\n",
    "The core of Semantic Kernel is it's Kernel. It's responsible to transform an User Input (or ASK) into a response by orchestrating Plugins. \n",
    "\n",
    "In this lab, we'll instantiate a simple Kernel with no additonal options. Optionally, we could define Logging, Memory and the set of skill we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "#Example of how to instantiate the kernel with a different log level\n",
    "#kernel = sk.Kernel(log=sk.Log(level=sk.LogLevel.DEBUG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Connect the Kernel to a Text Completion model and run a simple prompt\n",
    "Now that we have our kernel, we can start adding capabilities to it. \n",
    "\n",
    "In this lab, we'll connect to an **AI Service** and use a Text Completion model from **Azure OpenAI Service**. \n",
    "The Semantic Kernel also supports **OpenAI**. Other AI Services, like **Hugging Face** will be supported shortly. You can visit this [link](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages#ai-service-endpoints) to check the most updated list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish a connection between the Kernel and a Text Completion model, we first need to import **AzureTextCompletion** module and add it to the kernel as a Text Completion Service. \n",
    "\n",
    "We're also using a method to automatically read sensitive parameters from a `.env` file. Therefore, please ensure your file has the following parameters:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=\"NAME OF YOUR MODEL DEPLOYMENT\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://<YOUR_ENDPOINT>.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY=\"KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<semantic_kernel.kernel.Kernel at 0x17c2e1c5ed0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the Azure Text Completion service connector\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion\n",
    "\n",
    "# Read the model, API key and endpoint from the .env file\n",
    "deploynment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "\n",
    "# Add the text completion service to the kernel\n",
    "kernel.add_text_completion_service(\"Text Completion\", \n",
    "                                   AzureTextCompletion(\"text-davinci-003\", endpoint, api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have connected the kernel to a large language model from Azure OpenAI Service, we can start using it to run Text Completion operations.\n",
    "\n",
    "In Semantic Kernel we need to define a **Semantic Function** in order to run prompts using a LLM. We'll deep-dive on Semantic Functions later, so let's just focus on the basics in this step of the lab.\n",
    "\n",
    "You can add a **Semantic Function** to the Kernel based on a **Prompt**. This is simple a textual description, or instruction, of what we want the function to do. Optionally, we can add parameters, like `{{$input}}`, to inject values in runtime.\n",
    "\n",
    "In this example, we'll create a function to classify a Telecom problem in different categories, like Mobile Internet, Fixed Internet, etc. As **input**, we'll use the problem description. Please note that as we define the function we can also control the behavior of how we expect the kernel to run it, for example, controlling the max number of tokens in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt for classifying the problem description\n",
    "prompt = \"\"\"{{$input}}\n",
    "Classify the problem description above as a [Fixed Internet] or [Mobile Internet] category.\n",
    "Only write the category with no extra text.\n",
    "\"\"\"\n",
    "\n",
    "# Create a semantic function for classifying the problem description\n",
    "classify_call_function = kernel.create_semantic_function(prompt, \n",
    "                                                         max_tokens=10,\n",
    "                                                         temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Kernel has a new function to classify problems, we'll test it based on a problem description. We expected as a result one of the classifications previously defined in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: \n",
      "Fixed Internet\n"
     ]
    }
   ],
   "source": [
    "problem_description = \"My wi-fi connection is terrible at my bedroom.\"\n",
    "\n",
    "# Call the classify function with a problem description\n",
    "classification = classify_call_function(problem_description)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Result: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Connect the Kernel to a Chat Completion model and run a simple prompt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
